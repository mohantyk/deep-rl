{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap13-A3C.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gLuYq5bjRRRN",
        "0SHZadxzHwNZ",
        "1Zl4m5RZJPYd",
        "bOUydFC3nt-o"
      ],
      "authorship_tag": "ABX9TyM/1d3nhpNjeQATIEdOy0GA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohantyk/deep-rl/blob/master/Chap13_A3C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_bx4wLF7eJz"
      },
      "source": [
        "### Warning\n",
        "torch.multiprocessing does not seem to work with notebooks, see pytorch [issue](https://github.com/pytorch/pytorch/issues/17680). Need to run it as a script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7d2N1PK78KP"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_dr57N7aJgC",
        "outputId": "e4f669db-983f-4cb3-8743-b35bd3673f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install ptan tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ptan in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from ptan) (0.17.2)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (from ptan) (0.2.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from ptan) (4.1.2.30)\n",
            "Requirement already satisfied: torch==1.3.0 in /usr/local/lib/python3.6/dist-packages (from ptan) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptan) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WXh8t5XaQY_"
      },
      "source": [
        "import gym\n",
        "import ptan\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKW_X521HZ_v"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as nn_utils\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha0Bz0ZURx6g"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import sys\n",
        "import collections"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLuYq5bjRRRN"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn19MAWYRTnW"
      },
      "source": [
        "class RewardTracker:\n",
        "  def __init__(self, writer, stop_reward):\n",
        "    self.writer = writer\n",
        "    self.stop_reward = stop_reward\n",
        "\n",
        "  def __enter__(self):\n",
        "    self.ts = time.time()\n",
        "    self.ts_frame = 0\n",
        "    self.total_rewards = []\n",
        "    return self\n",
        "\n",
        "  def __exit__(self):\n",
        "    self.writer.close()\n",
        "\n",
        "  def reward(self, reward, frame, epsilon=None):\n",
        "    self.total_rewards.append(reward)\n",
        "    speed = (frame - self.ts_frame)/(time.time() - self.ts)\n",
        "    self.ts_frame = frame\n",
        "    self.ts = time.time()\n",
        "    mean_reward = np.mean(self.total_rewards[-100:])\n",
        "    epsilon_str = '' if epsilon is None else f', eps {epsilon:.2f}'\n",
        "    print(f'{frame} done {len(self.total_rewards)} games, mean reward {mean_reward:.3f}'\n",
        "          f', speed {speed:.2f} f/s{epsilon_str}')\n",
        "    sys.stdout.flush()\n",
        "    if epsilon is not None:\n",
        "      self.writer.add_scalar('epsilon', epsilon, frame)\n",
        "    self.writer.add_scalar('speed', speed, frame)\n",
        "    self.writer.add_scalar('reward_100', mean_reward, frame)\n",
        "    self.writer.add_scalar('reward', reward, frame)\n",
        "    if mean_reward > self.stop_reward:\n",
        "      print(f'Solved in {frame} frames!')\n",
        "      return True\n",
        "    return False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SHZadxzHwNZ"
      },
      "source": [
        "## HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67aVJgzEHlK9"
      },
      "source": [
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.001\n",
        "ENTROPY_BETA = 0.01\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "REWARD_STEPS = 4\n",
        "CLIP_GRAD = 0.1\n",
        "\n",
        "PROCESSES_COUNT = 2\n",
        "NUM_ENVS = 8\n",
        "MICRO_BATCH_SIZE = 32"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac_epSERQjhq"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "device = torch.device(DEVICE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zl4m5RZJPYd"
      },
      "source": [
        "# A3C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlSx-FTDH8_3"
      },
      "source": [
        "class AtariA2C(nn.Module):\n",
        "  def __init__(self, input_shape, n_actions):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    conv_out_size = self._get_conv_out(input_shape)\n",
        "    self.policy = nn.Sequential(\n",
        "        nn.Linear(conv_out_size, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, n_actions)\n",
        "    )  \n",
        "    self.value = nn.Sequential(\n",
        "        nn.Linear(conv_out_size, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def _get_conv_out(self, shape):\n",
        "    o = self.conv(torch.zeros(1, *shape))\n",
        "    return int(np.prod(o.size()))\n",
        "\n",
        "  def forward(self, x):\n",
        "    fx = x.float()/256\n",
        "    conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
        "    return self.policy(conv_out), self.value(conv_out)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FaKJTIrNGb0"
      },
      "source": [
        "def unpack_batch(batch, net, device):\n",
        "  '''\n",
        "  Converts batch into training tensors\n",
        "  returns :\n",
        "    training states, actions tensor, reference values\n",
        "  '''\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  not_done_idx = []\n",
        "  last_states = []\n",
        "  for idx, exp in enumerate(batch):\n",
        "    states.append(np.array(exp.state, copy=False))\n",
        "    actions.append(int(exp.action))\n",
        "    rewards.append(exp.reward)\n",
        "    if exp.last_state is not None:\n",
        "      not_done_idx.append(idx)\n",
        "      last_states.append(np.array(exp.last_state, copy=False))\n",
        "\n",
        "  states_v = torch.FloatTensor(np.array(states, copy=False)).to(device)\n",
        "  actions_t = torch.LongTensor(actions).to(device)\n",
        "  \n",
        "  rewards_np = np.array(rewards, dtype=np.float32)\n",
        "  if not_done_idx:\n",
        "    last_states_v = torch.FloatTensor(np.array(last_states, copy=False)).to(device)\n",
        "    last_vals_v = net(last_states_v)[1]\n",
        "    last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n",
        "    last_vals_np *= GAMMA ** REWARD_STEPS\n",
        "    rewards_np[not_done_idx] += last_vals_np\n",
        "  ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n",
        "  return states_v, actions_t, ref_vals_v"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOUydFC3nt-o"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5y7I0bFoA3s"
      },
      "source": [
        "def make_env():\n",
        "  return ptan.common.wrappers.wrap_dqn(gym.make('PongNoFrameskip-v4'))\n",
        "\n",
        "TotalReward = collections.namedtuple('TotalReward', ['reward'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gP1cXkJoKz6"
      },
      "source": [
        "def data_func(net, device, train_queue):\n",
        "  '''\n",
        "  Will run in child processes\n",
        "  '''\n",
        "  envs = [make_env() for _ in range(NUM_ENVS)]\n",
        "  agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], apply_softmax=True, \n",
        "                                 device=device)\n",
        "  exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, \n",
        "                                                       steps_count=REWARD_STEPS)\n",
        "  \n",
        "  micro_batch = []\n",
        "  for exp in exp_source:\n",
        "    new_rewards = exp_source.pop_total_rewards()\n",
        "    if new_rewards:\n",
        "      data = TotalReward(reward=np.mean(new_rewards))\n",
        "      train_queue.put(data)\n",
        "    \n",
        "    micro_batch.append(exp)\n",
        "    if len(micro_batch) < MICRO_BATCH_SIZE:\n",
        "      continue\n",
        "\n",
        "    data = unpack_batch(micro_batch, net, device=device)\n",
        "    train_queue.put(data)\n",
        "    micro_batch.clear()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOufDpd3wFE6"
      },
      "source": [
        "mp.set_start_method('spawn')\n",
        "os.environ['OMP_NUM_THREADS'] = \"1\" # Since we are implementing our own parallelism\n",
        "                                    # stop OpenMP from generating its own threads"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZd4SvfCron3"
      },
      "source": [
        "writer = SummaryWriter(comment='-pong-a3c-data')\n",
        "env = make_env()\n",
        "net = AtariA2C(env.observation_space.shape, env.action_space.n).to(device)\n",
        "net.share_memory() # Only required for CPUs, GPU tensors are shared by default\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
        "\n",
        "train_queue = mp.Queue(maxsize=PROCESSES_COUNT) # To transfer data from children\n",
        "data_proc_list = []\n",
        "for _ in range(PROCESSES_COUNT):\n",
        "  data_proc = mp.Process(target=data_func, args=(net, device, train_queue))\n",
        "  data_proc.start()\n",
        "  data_proc_list.append(data_proc)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhSdp-ff4E9",
        "outputId": "0323e0cc-e725-4b8b-9560-5a0cba1b6f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 284), started 0:15:50 ago. (Use '!kill 284' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_HZp5nwfcZ",
        "outputId": "920b4105-990c-4d31-c95a-dd3706e904ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "batch_states = []\n",
        "batch_actions = []\n",
        "batch_vals_ref = []\n",
        "step_idx = 0\n",
        "batch_size = 0\n",
        "\n",
        "try:\n",
        "  with RewardTracker(writer, stop_reward=17) as tracker:\n",
        "    with ptan.common.utils.TBMeanTracker(writer, 100) as tb_tracker:\n",
        "      while True:\n",
        "        train_entry = train_queue.get()\n",
        "        if isinstance(train_entry, TotalReward):\n",
        "          if tracker.reward(train_entry.reward, step_idx): # True only if step_reward exceeded\n",
        "            break\n",
        "          continue \n",
        "        # Else if state/action tensors\n",
        "        states_t, actions_t, vals_ref_t = train_entry\n",
        "        batch_states.append(states_t)\n",
        "        batch_actions.append(actions_t)\n",
        "        batch_vals_ref.append(vals_ref_t)\n",
        "        step_idx += states_t.size()[0]\n",
        "        batch_size += states_t.size()[0]\n",
        "        if batch_size < BATCH_SIZE:\n",
        "          continue\n",
        "\n",
        "        states_v = torch.cat(batch_states)\n",
        "        actions_t = torch.cat(batch_actions)\n",
        "        vals_ref_v = torch.cat(batch_vals_ref)\n",
        "        batch_states.clear()\n",
        "        batch_actions.clear()\n",
        "        batch_vals_ref.clear()\n",
        "        batch_size = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_v, value_v = net(states_v)\n",
        "        loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_refs_v)\n",
        "\n",
        "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
        "        adv_v = vals_refs_v - value_v.detach()\n",
        "        log_p_a = log_prob_v[range(BATCH_SIZE), actions_t]\n",
        "        log_prob_actions_v = adv_v * log_p_a\n",
        "        loss_policy_v = -log_prob_actions_v.mean()\n",
        "\n",
        "        prob_v = F.softmax(logits_v, dim=1)\n",
        "        entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()\n",
        "\n",
        "        # Calculate policy gradients only\n",
        "        loss_policy_v.backward(retain_graph=True)\n",
        "        grads = np.concatenate([p.grad.data.cpu().numpy().flatten()\n",
        "                                for p in net.parameters()\n",
        "                                if p.grad is not None])\n",
        "        \n",
        "        # Apply entropy and value gradients\n",
        "        loss_v = entropy_loss_v + loss_value_v\n",
        "        loss_v.backward()\n",
        "        nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "\n",
        "        tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
        "        tb_tracker.track(\"values\", value_v, step_idx)\n",
        "        tb_tracker.track(\"batch_rewards\", vals_ref_v,\n",
        "                          step_idx)\n",
        "        tb_tracker.track(\"loss_entropy\",\n",
        "                          entropy_loss_v, step_idx)\n",
        "        tb_tracker.track(\"loss_policy\",\n",
        "                          loss_policy_v, step_idx)\n",
        "        tb_tracker.track(\"loss_value\",\n",
        "                          loss_value_v, step_idx)\n",
        "        tb_tracker.track(\"loss_total\",\n",
        "                          loss_v, step_idx)\n",
        "        \n",
        "finally:\n",
        "  for p in data_proc_list:\n",
        "    p.terminate()\n",
        "    p.join()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c434f30f4d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTotalReward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c434f30f4d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m                           loss_value_v, step_idx)\n\u001b[1;32m     70\u001b[0m         tb_tracker.track(\"loss_total\",\n\u001b[0;32m---> 71\u001b[0;31m                           loss_v, step_idx)\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __exit__() takes 1 positional argument but 4 were given"
          ]
        }
      ]
    }
  ]
}