{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap6-DeepQNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOd52ftwDKJmDlt+shdopYv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohantyk/deep-rl/blob/master/Chap6_DeepQNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FeT1KUz4MjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "fdf72d23-a25a-4bca-96a3-41643a71be8e"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpvweagu7chU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from collections import defaultdict\n",
        "from tensorboardX import SummaryWriter\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbhgrNPi7kaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URcBGXmT7oyc",
        "colab_type": "text"
      },
      "source": [
        "# FrozenLake Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q8OeHYi7nue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'FrozenLake-v0'\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.2\n",
        "TEST_EPISODES = 20"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDjyDttf70In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self.env = gym.make(ENV_NAME)\n",
        "    self.values = defaultdict(float) # key (s, a)\n",
        "    self.state = self.env.reset()\n",
        "\n",
        "  def sample_env(self):\n",
        "    '''Returns a (s,a,r,s') tuple'''\n",
        "    action = self.env.action_space.sample()\n",
        "    old_state = self.state\n",
        "    new_state, reward, is_done, _ = self.env.step(action)\n",
        "    self.state = self.env.reset() if is_done else new_state\n",
        "    return (old_state, action, reward, new_state)\n",
        "\n",
        "  def best_value_and_action(self, state):\n",
        "    '''Returns best action and its value in a given state'''\n",
        "    best_action, best_value = None, -math.inf\n",
        "    for action in range(self.env.action_space.n):\n",
        "      val = self.values[(state, action)]\n",
        "      if val > best_value:\n",
        "        best_value = val\n",
        "        best_action = action\n",
        "    return best_value, best_action\n",
        "\n",
        "  def value_update(self, s, a, r, next_s):\n",
        "    '''Update value of (s,a) given a (s,a,r,s') occurence'''\n",
        "    old_val = self.values[(s,a)]\n",
        "    best_value, _ = self.best_value_and_action(next_s)\n",
        "    new_val = r + GAMMA*best_value\n",
        "    self.values[(s, a)] = old_val*(1-ALPHA) + new_val * ALPHA\n",
        "\n",
        "  def play_episode(self, env):\n",
        "    '''Play test episode. Returns total reward for episode'''\n",
        "    total_reward = 0.0\n",
        "    state = env.reset()\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "      _, action = self.best_value_and_action(state)\n",
        "      new_state, reward, is_done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      state = new_state\n",
        "    return total_reward"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-k_TShB72lH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_env = gym.make(ENV_NAME)\n",
        "agent = Agent()\n",
        "writer = SummaryWriter(comment='-q-learning')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXIbvYJXDXOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a6afc51d-fc34-48a2-a688-94d65aed87d4"
      },
      "source": [
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "  iter_no += 1\n",
        "  state, action, reward, next_state = agent.sample_env()\n",
        "  agent.value_update(state, action, reward, next_state)\n",
        "\n",
        "  test_reward = 0.0\n",
        "  for _ in range(TEST_EPISODES):\n",
        "    test_reward += agent.play_episode(test_env)\n",
        "  test_reward /= TEST_EPISODES\n",
        "  writer.add_scalar('reward', test_reward, iter_no)\n",
        "  if test_reward > best_reward:\n",
        "    print(f'{iter_no}: Best reward updated from {best_reward} -> {test_reward}')\n",
        "    best_reward = test_reward\n",
        "  if test_reward > 0.8:\n",
        "    print(f'Solved in {iter_no} iterations')\n",
        "    break\n",
        "writer.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "433: Best reward updated from 0.05 -> 0.0\n",
            "434: Best reward updated from 0.15 -> 0.05\n",
            "587: Best reward updated from 0.25 -> 0.15\n",
            "610: Best reward updated from 0.3 -> 0.25\n",
            "1326: Best reward updated from 0.35 -> 0.3\n",
            "1512: Best reward updated from 0.4 -> 0.35\n",
            "2440: Best reward updated from 0.45 -> 0.4\n",
            "2797: Best reward updated from 0.6 -> 0.45\n",
            "2798: Best reward updated from 0.8 -> 0.6\n",
            "2933: Best reward updated from 0.9 -> 0.8\n",
            "Solved in 2933 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5puuVP-8K-Nb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "2903bc65-a59d-4cbe-8a98-cbb07c7d0426"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1057), started 4:17:43 ago. (Use '!kill 1057' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nde31XLyNh0G",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEY90dFEmvDh",
        "colab_type": "text"
      },
      "source": [
        "## Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stlDxaBLNkvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0XS1gltnqJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "  def __init__(self, env=None):\n",
        "    super().__init__(env)\n",
        "    assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "    assert len(env.unwrapped.get_action_meanings()) > 3\n",
        "\n",
        "  def step(self, action):\n",
        "    return self.env.step(action)\n",
        "\n",
        "  def reset(self):\n",
        "    '''Reset underlying environment and press FIRE and move right'''\n",
        "    self.env.reset()\n",
        "    obs, reward, is_done, _ = self.step(1) # Press FIRE button\n",
        "    if is_done: # Pressing FIRE was not such a good idea\n",
        "      obs, reward, is_done, _ = self.env.reset() \n",
        "    obs, reward, is_done, _ = self.env.step(2) # Move right? Why?\n",
        "    if is_done:\n",
        "      obs, reward, is_done, _ = self.env.reset()\n",
        "    return obs\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkYfRbDdpFve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "  '''\n",
        "  Repeats same action for K frames.\n",
        "  Takes max of pixels over 2 frames.\n",
        "  '''\n",
        "  def __init__(self, env=None, skip=4):\n",
        "    super().__init__(env)\n",
        "    self._obs_buffer = deque(maxlen=2) # To take max of pixels\n",
        "    self._skip = 4\n",
        "\n",
        "  def step(self, action):\n",
        "    '''\n",
        "    Take same action for skip frames.\n",
        "    When returning observation, take maximum of pixels for 2 frames\n",
        "    '''\n",
        "    total_reward = 0.0\n",
        "    for _ in range(self._skip):\n",
        "      obs, reward, is_done, info = self.env.step(action)\n",
        "      self._obs_buffer.append(obs)\n",
        "      total_reward += reward\n",
        "      if is_done:\n",
        "        break\n",
        "    max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "    return max_frame, total_reward, is_done, info\n",
        "\n",
        "  def reset(self):\n",
        "    self._obs_buffer.clear()\n",
        "    obs = self.env.reset()\n",
        "    self._obs_buffer.append(obs)\n",
        "    return obs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geC1bjaKrLJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "  def __init__(self, env=None):\n",
        "    super().__init__(env)\n",
        "    self.observation_space = gym.spaces.Box(low=0, high=255, \n",
        "                                            shape=(84,84,1), dtype=np.uint8)\n",
        "  \n",
        "  def observation(self, obs):\n",
        "    return ProcessFrame84.process(obs)\n",
        "\n",
        "  @staticmethod\n",
        "  def process(frame):\n",
        "    if frame.size == 210 * 160 * 3:\n",
        "      img = np.reshape(frame, (210, 160, 3)).astype(np.float32)\n",
        "    elif frame.size == 250 * 160 * 3:\n",
        "      img = np.reshape(frame, (250, 160, 3)).astype(np.float32)\n",
        "    else:\n",
        "      raise ValueError('Unknown resolution')\n",
        "\n",
        "    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, : , 2] * 0.114\n",
        "    resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "    x_t = resized_screen[18:102, :]\n",
        "    x_t = np.reshape(x_t, (84, 84, 1))\n",
        "    return x_t.astype(np.uint8) "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6RcnVPtsl-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "  '''\n",
        "  Buffers N frames into a single observation\n",
        "  Needed for obtaining info about dynamics\n",
        "  '''\n",
        "  def __init__(self, env, n_steps, dtype=np.float32):\n",
        "    super().__init__(env)\n",
        "    self.dtype = dtype\n",
        "    old_space = env.observation_space\n",
        "    self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                            old_space.high.repeat(n_steps, axis=0),\n",
        "                                            dtype=dtype)\n",
        "    \n",
        "  def reset(self):\n",
        "    self.buffer = np.zeros_like(self.observation_space.low)\n",
        "    return self.observation(self.env.reset())\n",
        "\n",
        "  def observation(self, obs):\n",
        "    '''Remove oldest observation. Append new observation at end of buffer'''\n",
        "    self.buffer[:-1] = self.buffer[1:]\n",
        "    self.buffer[-1] = obs\n",
        "    return self.buffer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j18J-COdz2F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "  '''\n",
        "  Converts height/width/channel to channel/height/width format\n",
        "  CHW format is needed for PyTorch convnets\n",
        "  '''\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    old_shape = env.observation_space.shape\n",
        "    new_shape = (old_shape[2], old_shape[0], old_shape[1])\n",
        "    self.observation_space = gym.spaces.Box(low=0.0, high=1.0, \n",
        "                                            shape=new_shape, dtype=np.float32)\n",
        "    \n",
        "  def observation(self, obs):\n",
        "    return np.moveaxis(obs, 2, 0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYMolwng2tup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "  '''\n",
        "  Scales observations to [0,1] range\n",
        "  '''\n",
        "  def observation(self, obs):\n",
        "    return np.array(obs).astype(np.float32)/255.0"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDiWJTNM3mpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_env(env_name):\n",
        "  env = gym.make(env_name)\n",
        "  env = MaxAndSkipEnv(env)\n",
        "  env = FireResetEnv(env)\n",
        "  env = ProcessFrame84(env)\n",
        "  env = ImageToPyTorch(env)\n",
        "  env = BufferWrapper(env, 4)\n",
        "  env = ScaledFloatFrame(env)\n",
        "\n",
        "  return env"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pCv4GVs4HB4",
        "colab_type": "text"
      },
      "source": [
        "## DQN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmkQFDOC4E-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfUJKoNeIdG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, input_shape, num_actions):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
        "            nn.ReLU()\n",
        "    )\n",
        "    # Need to know the shape of the conv network output\n",
        "    # Note: this function is called only once\n",
        "    conv_out_size = self._get_conv_out(input_shape)\n",
        "    self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "    )\n",
        "\n",
        "  def _get_conv_out(self, input_shape):\n",
        "    # Create a dummy input of zeros and run through convnet\n",
        "    o = self.conv(torch.zeros(1, *input_shape))\n",
        "    return int(np.prod(o.size()))\n",
        "\n",
        "  def forward(self, x):\n",
        "    conv_out = self.conv(x)\n",
        "    # Convert each observation into a single row.\n",
        "    # Reshape into 2-dims where dim[0] is batch_size\n",
        "    batch_size = x.shape[0]\n",
        "    fc_in = conv_out.view(batch_size, -1)\n",
        "    # Run through fully connected network\n",
        "    return self.fc(fc_in)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWZXKie0MYAi",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc3PIEFhMJB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_ENV_NAME = 'PongNoFrameskip-v4'\n",
        "MEAN_REWARD_BOUND = 19.0\n",
        "# Training Parameters\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32 \n",
        "REPLAY_SIZE = 10000 # Maximum capacity of replay buffer\n",
        "REPLAY_START_SIZE = 10000 # The count of frames we wait to start training\n",
        "LEARNING_RATE = 1e-4 # Adam learning rate\n",
        "SYNC_TARGET_FRAMES = 1000 # How often sync weights from traning model to target model\n",
        "# Epsilon Decay parameters\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.1\n",
        "EPSILON_DECAY_LAST_FRAME = 150000 # linearly decay epsilon\n",
        "# Device\n",
        "DEVICE = 'cuda'\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8svQBiwofzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Experience = namedtuple('Experience', \n",
        "                        ['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "    \n",
        "  def sample(self, batch_size):\n",
        "    indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "    states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "    return np.array(states), np.array(actions), \\\n",
        "           np.array(rewards, dtype=np.float32), \\\n",
        "           np.array(dones, dtype=np.uint8), \\\n",
        "           np.array(next_states)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4YXIdKoqMJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7871c833-5980-4e5e-a5e2-7f7afd8d6d84"
      },
      "source": [
        "# Small code snippet to demonstrate what is happening in sample() above\n",
        "a = [(1,2,3), (10,20,30), (100,200,300)]\n",
        "indices = [0, 1, 2]\n",
        "x, y, z = zip(*[a[idx] for idx in indices])\n",
        "x"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjU78FbFrCX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self, env, exp_buffer):\n",
        "    self.env = env\n",
        "    self.exp_buffer = exp_buffer\n",
        "    self._reset()\n",
        "  \n",
        "  def _reset(self):\n",
        "    self.state = self.env.reset()\n",
        "    self.total_reward = 0.0\n",
        "\n",
        "  @torch.no_grad()  # Forces requires_grad = False for inference\n",
        "  def play_step(self, net, epsilon=0.0, device='cuda'):\n",
        "    done_reward = None # Reward only after episode is complete\n",
        "    # Choose action e-greedily\n",
        "    if np.random.random() < epsilon:\n",
        "      action = self.env.action_space.sample()\n",
        "    else:\n",
        "      obs = np.array([self.state], copy=False)\n",
        "      obs_v = torch.tensor(obs)\n",
        "      q_vals = net(obs_v)\n",
        "      max_q, max_idx = torch.max(q_vals, dim=1)\n",
        "      action = max_idx.item() # max_idx is a tensor\n",
        "    # Take step\n",
        "    new_state, reward, done, _ = self.env.step(action)\n",
        "    self.total_reward += reward\n",
        "    # Save experience to replay buffer\n",
        "    exp = Experience(self.state, action, reward, done, new_state) \n",
        "    self.exp_buffer.append(exp)\n",
        "    # Update state, handle end of episode\n",
        "    self.state = new_state\n",
        "    if done:\n",
        "      done_reward = self.total_reward\n",
        "      self._reset()\n",
        "    return done_reward\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoEX6cxs0QeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_loss(batch, net, tgt_net, device='cuda'):\n",
        "  '''\n",
        "  batch: tuple of arrays (output of ExperienceBuffer.sample())\n",
        "  net: network we are training\n",
        "  tgt_net: target network (for target estimates)\n",
        "  '''\n",
        "  states, actions, rewards, dones, next_states = batch # Each is numpy array\n",
        "  # Convert to tensors on appropriate device\n",
        "  states_v = torch.tensor(states).to(device)\n",
        "  next_states_v = torch.tensor(next_states).to(device)\n",
        "  actions_v = torch.tensor(actions).to(device)\n",
        "  rewards_v = torch.tensor(rewards).to(device)\n",
        "  done_mask = torch.BoolTensor(dones).to(device)\n",
        "  # Predict action values\n",
        "  state_action_vals = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "  # Calculated expected action values\n",
        "  next_state_act_vals = tgt_net(next_states_v)\n",
        "  next_state_vals, _ = next_state_act_vals.max(1) # ignore max indices\n",
        "  next_state_vals[done_mask] = 0.0 # if episode is done, only need to add reward\n",
        "  # Stop backpropagation from flowing back into tgt_net\n",
        "  next_state_vals = next_state_vals.detach() \n",
        "  expected_state_vals = rewards_v + GAMMA * next_state_vals\n",
        "  # Calculate loss\n",
        "  mse = nn.MSELoss()\n",
        "  loss = mse(state_action_vals, expected_state_vals)\n",
        "  return loss"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9QBDBV6WyWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "87e1bdae-b47c-4291-da43-0723f903494b"
      },
      "source": [
        "device = torch.device(DEVICE)\n",
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment='-'+DEFAULT_ENV_NAME)\n",
        "print(net)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U8hL-23YIue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Agent\n",
        "replay_buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "epsilon = EPSILON_START\n",
        "agent = Agent(env, replay_buffer)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuW1apkQZR0e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "34905149-003a-46c0-9af6-ad1f80fd082f"
      },
      "source": [
        "# Create optimizer and counters\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_mean_reward = -math.inf\n",
        "\n",
        "# Main training loop\n",
        "while True:\n",
        "  frame_idx += 1\n",
        "  epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "  reward = agent.play_step(net, epsilon, device)\n",
        "  # If episode finished, calculate a bunch of indicators\n",
        "  if reward is not None: \n",
        "    total_rewards.append(reward)\n",
        "    speed = (frame_idx - ts_frame)/(time.time() - ts)\n",
        "    ts_frame = frame_idx\n",
        "    ts = time.time()\n",
        "    mean_reward = np.mean(total_rewards[-100:])\n",
        "    print(f'{frame_idx}: done {len(total_rewards)} games, reward {mean_reward:.3f},\\\n",
        "            epsilon {epsilon:.2f} speed {speed:.2f} f/s')\n",
        "\n",
        "    writer.add_scalar('epsilon', epsilon, frame_idx)\n",
        "    writer.add_scalar('speed', speed, frame_idx)\n",
        "    writer.add_scalar('reward_100', mean_reward, frame_idx)\n",
        "    writer.add_scalar('reward', reward, frame_idx)\n",
        "\n",
        "    if mean_reward > best_mean_reward:\n",
        "      print(f'Best reward updated {best_mean_reward:.3f} -> {mean_reward:.3f}')\n",
        "      best_mean_reward = mean_reward\n",
        "\n",
        "    if mean_reward > MEAN_REWARD_BOUND:\n",
        "      print(f'Solved in {frame_idx} frames!')\n",
        "      break\n",
        "\n",
        "  # Wait for minimum buffer size before starting to train\n",
        "  if len(replay_buffer) < REPLAY_START_SIZE:\n",
        "    continue\n",
        "  # Sync target net and training net \n",
        "  if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "    tgt_net.load_state_dict(net.state_dict())\n",
        "  # Train networks\n",
        "  optimizer.zero_grad()\n",
        "  batch = replay_buffer.sample(BATCH_SIZE)\n",
        "  loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "  loss_t.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1029: done 1 games, reward -21.000,            epsilon 0.99 speed 567.43 f/s\n",
            "Best reward updated -inf -> -21.0\n",
            "2231: done 2 games, reward -20.000,            epsilon 0.99 speed 568.83 f/s\n",
            "Best reward updated -21.0 -> -20.0\n",
            "3290: done 3 games, reward -20.333,            epsilon 0.98 speed 534.12 f/s\n",
            "4225: done 4 games, reward -20.000,            epsilon 0.97 speed 531.33 f/s\n",
            "5167: done 5 games, reward -19.800,            epsilon 0.97 speed 538.59 f/s\n",
            "Best reward updated -20.0 -> -19.8\n",
            "6222: done 6 games, reward -19.667,            epsilon 0.96 speed 527.97 f/s\n",
            "Best reward updated -19.8 -> -19.666666666666668\n",
            "7042: done 7 games, reward -19.857,            epsilon 0.95 speed 533.04 f/s\n",
            "7920: done 8 games, reward -20.000,            epsilon 0.95 speed 535.34 f/s\n",
            "8936: done 9 games, reward -20.000,            epsilon 0.94 speed 512.61 f/s\n",
            "9777: done 10 games, reward -20.000,            epsilon 0.93 speed 519.82 f/s\n",
            "10586: done 11 games, reward -20.091,            epsilon 0.93 speed 14.22 f/s\n",
            "11408: done 12 games, reward -20.167,            epsilon 0.92 speed 9.60 f/s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}